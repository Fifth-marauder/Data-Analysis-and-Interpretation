---
title: "Data Analysis (Chi-Squared) - R"
author: "Sarah Pohl"
date: "30. November 2015"
output: 
  html_document: 
    keep_md: yes
---

```{r Prep, echo=FALSE}
#setwd("C:/Users/Sarah/Dropbox/coursera/Data Analysis and Interpretation/DataAnaT")
setwd("C:/Users/spo12/Dropbox/coursera/Data Analysis and Interpretation/DataAnaT")
options(stringsAsFactors=FALSE)
```

As [mentioned before](http://lilithelina.tumblr.com/post/128638794919/choice-of-language), I want to compare Python and R analysis steps in the [DataManViz](http://lilithelina.tumblr.com/tagged/DataManViz) and now [DataAnaT](http://lilithelina.tumblr.com/tagged/DataAnaT) projects, so this is the R version of the [Data Analysis - Chi-Square](http://lilithelina.tumblr.com/post/133785457799/data-analysis-chi-square-python) Python script. Again, the whole thing will look better over [here](http://htmlpreview.github.io/?https://github.com/LilithElina/Data-Analysis-and-Interpretation/blob/master/DataAnaT/Week_Two_Chi.html).

I will first run some of my previous code to remove variables I don't need and observations for which important data is missing.

```{r Past, tidy=TRUE, comment="", message=FALSE, warning=FALSE}
# load libraries
library(Hmisc)   # for cut2()
library(reshape) # for melt()
library(ggplot2) # for the plot

# load data
gapminder <- read.table("../gapminder.csv", sep=",", header=TRUE, quote="\"")

# subset data
sub_data <- subset(gapminder, select=c("country", "breastcancerper100th", "femaleemployrate", "internetuserate"))

# remove rows with NAs
sub_data2 <- na.omit(sub_data)
```

The $\chi^2$ test checks if two categorical variables are independent of each other - meaning the distribution of one is not affected by the other. Therefore, I have to categorise my continuous variables; I did this with the breast cancer cases for [ANOVA](http://lilithelina.tumblr.com/post/132994745429/data-analysis-anova-r), and I'll stick to the four equal sized groups, while female employment and internet usage will both be split into two groups (to make the results easier to interpret).

```{r Group, tidy=TRUE, comment="", results='hold'}
# split breast cancer cases into four groups
sub_data2$breastQuart <- cut2(sub_data2$breastcancerper100th, g=4)

# split female employment and internet usage in two groups
sub_data2$employtwo <- cut2(sub_data2$femaleemployrate, g=2)
sub_data2$interntwo <- cut2(sub_data2$internetuserate, g=2)

cat("breast cancer cases per 100,000 females - quartiles")
table(sub_data2$breastQuart)
cat("female employment rate - two groups")
table(sub_data2$employtwo)
cat("internet use rate - two groups")
table(sub_data2$interntwo)

# melt data into long format
sub_data2.m <- melt(sub_data2, id.vars="breastQuart", measure.vars=c("employtwo", "interntwo"))

# plot bar charts of low and high female employment or internet usage for the four breast cancer groups/quartiles
ggplot(sub_data2.m, aes(x=breastQuart)) +
  geom_bar(aes(fill=value), position="dodge") + facet_wrap(~ variable) +
  theme(axis.ticks = element_blank(), axis.text.x=element_text(angle=90)) +
  scale_fill_hue(l=50)
```

While the groups have similar sizes to those I created in Python, the bar chart shows counts for both categories of female employment and internet usage, respectively. This way, we can easily see that countries from the lowest breast cancer quartile mostly have higher female employment rates (green), while the next two quartiles show more lower than higher employment rates (red). Only in the fourth group is the distribution almost balanced.  
The internet use rates show a much clearer distinction. While low internet usage goes down with increasing breast cancer prevalence (blue-green), high internet usage goes up (purple). We've seen in the analysis of variance that these differences are significant. Before we test this again with the $\chi^2$ test, let's have a look at the contingency tables.

```{r Contingency, tidy=TRUE, comment="", results="hold"}
expected <- data.frame("25th"=c((41*81)/162, (41*81)/162),
                       "50th"=c((40*81)/162, (40*80)/162),
                       "75th"=c((41*81)/162, (41*81)/162),
                       "100th"=c((40*81)/162, (40*81)/162))
employ_cont <- table(sub_data2$employtwo, sub_data2$breastQuart)
intern_cont <- table(sub_data2$interntwo, sub_data2$breastQuart)

cat("expected values in the breast cancer quartiles\n")
print(expected)
cat("\nobserved values for female employment groups")
print(employ_cont)
cat("\nobserved values for internet usage groups")
print(intern_cont)
```

These also look very similar to those we got in [Python](http://lilithelina.tumblr.com/post/133785457799/data-analysis-chi-square-python) - and they differ greatly from the expected values.  
The contingency tables are now used as input for the `chisq.test()` function.

```{r Chi, tidy=TRUE, comment="", results="hold"}
cat("chi-squared test for female employment and breast cancer")
chisq.test(employ_cont)
cat("chi-squared test for internet usage and breast cancer")
chisq.test(intern_cont)
```

The output is a bit nicer here than in Python, and the $\chi^2$ values are slightly different (20.99 versus 20.33 and 73.66 versus 74.14), due to the slightly different categorisation of the breast cancer cases. Of course, the results are still significant, so there is a dependence between breast cancer and female employment or internet usage. As before, we can get more specific by comparing the breast cancer quartiles one-on-one.  
I'm not a fan of using `for` loops in R, but I couldn't think of anything else so far. I'd be delighted to see your ideas!

```{r ChiSingle, tidy=TRUE, comment="", results="hold"}
cat("breast cancer versus female employment\n")
for (x in 1:3) {
  for (y in (x+1):4) {
    test <- chisq.test(employ_cont[, c(x,y)])
    cat("group", x, "versus group", y, "\n")
    cat("Chi value:\t", test$statistic[[1]], "p value:\t", test$p.value, "\n")
  }
}

cat("\nbreast cancer versus internet usage\n")
for (x in 1:3) {
  for (y in (x+1):4) {
    test <- chisq.test(intern_cont[, c(x,y)])
    cat("group", x, "versus group", y, "\n")
    cat("Chi value:\t", test$statistic[[1]], "p value:\t", test$p.value, "\n")
  }
}
```

Again, we can't take these *p*-values at face value ([multiple comparisons](https://en.wikipedia.org/wiki/Multiple_comparisons_problem)...). Instead of declaring everything below $p=0.05$ as significant, we follow [Bonferroni](https://en.wikipedia.org/wiki/Bonferroni_correction) and divide that by the number of tests we've run: $0.05/12 = 0.0042$. We can be confident when rejecting the null hypothesis of every test with a *p*-value lower than that.

Exploring the possible independence of female employment rates from 2007 and new breast cancer cases in 2002 in different countries, we can see that there likely is a dependence between the two variables ($\chi^2 = 20.99$, 3 degrees of freedom, $p = 0.0001$). The single comparisons revealed that the null hypothesis of independence can only be rejected when comparing the first breast cancer group (low breast cancer prevalence) with the two middle groups, though.

Internet usage (from 2010) also depends on breast cancer ($\chi^2 = 73.66$, 3 degrees of freedom, $p = 7.016e-16$). Here, only the comparison of the first two breast cancer groups did not show a significantly different distribution of internet use rates. In all other comparisons, the dependence of the two variables was corroborated.